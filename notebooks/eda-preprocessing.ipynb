{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install requirementss\n",
    "!pip install -r ../requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from meteostat import Point, Daily\n",
    "from datetime import datetime, timedelta\n",
    "import os   \n",
    "import shutil\n",
    "import kagglehub\n",
    "import holidays\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Download Kaggle Dataset\n",
    "this contains 3 million rows,\n",
    "- We will just use one year of data, so from 31-07-2024 onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download to cache\n",
    "path = kagglehub.dataset_download(\"patrickzel/flight-delay-and-cancellation-dataset-2019-2023\")\n",
    "\n",
    "# Choose your desired location\n",
    "target_dir = \"../data/raw\"  # relative to your project\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Copy all downloaded files\n",
    "for file in os.listdir(path):\n",
    "    shutil.copy(os.path.join(path, file), os.path.join(target_dir, file))\n",
    "\n",
    "print(\"Dataset copied to:\", target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.max_columns = 99  # Show all columns in DataFrame  \n",
    "pd.max_rows = 99  # Show all rows in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/flights_sample_3m.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show last 15 columns where arr_dleay is greater than 0    \n",
    "len(df[df[\"ARR_DELAY\"] > 60].iloc[:, -15:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and filter dataset for rows after 31-07-2024\n",
    "df_reduced = df[df['FL_DATE'] >= '2021-07-31']\n",
    "df.shape, df_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now save this back\n",
    "df_reduced.to_csv(\"../data/raw/flights_sample_reduced.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functiont o do this # load and filter dataset for rows after 31-07-2024\n",
    "def load_and_filter_dataset(file_path, start_date):\n",
    "    \"\"\"Loads the dataset and filters for rows after selected date.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"FL_DATE\"] = pd.to_datetime(df[\"FL_DATE\"])\n",
    "    filtered_df = df[df[\"FL_DATE\"] >= start_date]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = load_and_filter_dataset(\"../data/raw/flights_sample_3m.csv\",\"2022-07-31\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Now begin exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the columns\n",
    "df_reduced.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Here we use openflight to get data about airports such as long/ lat to enable mapping to weather data later\n",
    "- first we svae this locally and then convert from .dat to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenFlights columns\n",
    "cols = [\n",
    "    \"airport_id\",\"name\",\"city\",\"country\",\"IATA\",\"ICAO\",\n",
    "    \"lat\",\"lon\",\"alt\",\"timezone\",\"dst\",\"tz_db\",\"type\",\"source\"\n",
    "]\n",
    "air = pd.read_csv(\"../test_data/data/raw/airports.dat\", names=cols)\n",
    "\n",
    "# Keep only needed columns & drop bad rows\n",
    "air = air[[\"IATA\",\"ICAO\",\"name\",\"lat\",\"lon\",\"tz_db\"]]\n",
    "air = air.dropna(subset=[\"IATA\",\"lat\",\"lon\",\"tz_db\"])\n",
    "air = air[air[\"IATA\"] != \"\\\\N\"].drop_duplicates(\"IATA\")\n",
    "\n",
    "# Save cleaned CSV\n",
    "air.to_csv(\"../data/raw/airports.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(air)} airports to data/raw/airports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dont want diverted or cancelled flights\n",
    "def removed_diverted_or_cancelled(df):\n",
    "    \"\"\"\n",
    "    Remove diverted or cancelled flights from the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing flight data.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered DataFrame with diverted and cancelled flights removed.\n",
    "    \"\"\"\n",
    "    return df[(df['DIVERTED'] == 0) & (df['CANCELLED'] == 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = removed_diverted_or_cancelled(df_reduced)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to create a delay flag for 15 and 30 minutes delay\n",
    "def create_delay_flag(df, delay_threshold=30):\n",
    "    \"\"\"\n",
    "    Create a delay flag based on the specified delay threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing flight data.\n",
    "    delay_threshold (int): Delay threshold in minutes to flag as delayed.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with an additional 'DELAY_FLAG' column.\n",
    "    \"\"\"\n",
    "    df[f'DELAY_FLAG_{delay_threshold}'] = df['ARR_DELAY'].apply(lambda x: 1 if x >= delay_threshold else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_delay_flag(df, delay_threshold=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class balance\n",
    "df['DELAY_FLAG_30'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Now we will add the weather data\n",
    "we will use meteosat\n",
    "✈️ Flight Weather Enrichment Pipeline — High-Level Steps\n",
    "\n",
    "1. Input & Requirements\n",
    "\t•\tTakes a flights dataset (BTS-style) with:\n",
    "\t•\tFL_DATE, ORIGIN, DEST, CRS_DEP_TIME, CRS_ARR_TIME\n",
    "\t•\tNeeds an airports CSV mapping IATA codes → latitude & longitude.\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Create Local Time Keys\n",
    "\t•\tConvert CRS_DEP_TIME & CRS_ARR_TIME (HHMM format) into hourly timestamps in local flight date.\n",
    "\t•\tTwo keys created:\n",
    "\t•\tdep_hour_dt → planned departure hour\n",
    "\t•\tarr_hour_dt → planned arrival hour\n",
    "\n",
    "⸻\n",
    "\n",
    "3. Build Unique Weather Requests\n",
    "\t•\tCombine all (airport, hour) pairs from dep & arr flights.\n",
    "\t•\tExtract year from each timestamp.\n",
    "\t•\tBuild a list of (IATA, year) pairs → minimum needed weather calls.\n",
    "\n",
    "⸻\n",
    "\n",
    "4. Fetch & Cache Raw Weather Data\n",
    "\t•\tFor each (IATA, year):\n",
    "\t1.\tCheck local cache (weather_cache/IATA_YEAR_RAW.parquet).\n",
    "\t2.\tIf cached → load it.\n",
    "\t3.\tIf not:\n",
    "\t•\tUse Meteostat Hourly API to fetch:\n",
    "\t•\ttemp (°C), prcp (mm), wspd (km/h)\n",
    "\t•\tSave raw data to cache (keeps API usage low).\n",
    "\t•\tUses multi-threading to fetch multiple airports in parallel.\n",
    "\t•\tShows a progress bar while fetching.\n",
    "\n",
    "⸻\n",
    "\n",
    "5. Compute Weather Flags\n",
    "\n",
    "From the raw weather data, create binary flags:\n",
    "\t•\train → precipitation > 0\n",
    "\t•\tice  → temperature ≤ 0°C and precipitation > 0\n",
    "\t•\twind → wind speed ≥ wind_kmh_threshold (user-configurable)\n",
    "\n",
    "Because the cache stores raw data, you can change the threshold and recompute without re-fetching.\n",
    "\n",
    "⸻\n",
    "\n",
    "6. Merge Weather into Flights\n",
    "\t•\tFor departures: join on (ORIGIN, dep_hour_dt)\n",
    "\t•\tFor arrivals: join on (DEST, arr_hour_dt)\n",
    "\t•\tAdd:\n",
    "\t•\tdep_rain, dep_ice, dep_wind\n",
    "\t•\tarr_rain, arr_ice, arr_wind\n",
    "\t•\tAlso add *_missing flags to mark where weather data wasn’t available.\n",
    "\n",
    "⸻\n",
    "\n",
    "7. Output\n",
    "\t•\tReturns the original flights DataFrame with 6 weather flags + 6 missing flags added.\n",
    "\t•\tEfficient (fetches once per airport/year), reusable (cache), and configurable.\n",
    "\n",
    "⸻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Weather Enrichment (helpers + orchestrator) =========\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")  # silence pandas FutureWarnings\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "from meteostat import Hourly, Point\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# ----------------------------- #\n",
    "# Low-level helpers (pure funcs)\n",
    "# ----------------------------- #\n",
    "\n",
    "def _hhmm_to_hour(x) -> int:\n",
    "    \"\"\"Convert HHMM-ish values (930, 0930, '1730', 930.0) to an integer hour [0..23].\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return 0\n",
    "    s = str(int(float(x))).zfill(4)[:4]\n",
    "    return int(s[:2])\n",
    "\n",
    "\n",
    "def _add_hour_keys(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add dep_hour_dt / arr_hour_dt columns (local-date + hour buckets).\"\"\"\n",
    "    out = df.copy()\n",
    "    out[\"FL_DATE\"] = pd.to_datetime(out[\"FL_DATE\"])\n",
    "    dep_h = out[\"CRS_DEP_TIME\"].apply(_hhmm_to_hour)\n",
    "    arr_h = out[\"CRS_ARR_TIME\"].apply(_hhmm_to_hour)\n",
    "    out[\"dep_hour_dt\"] = pd.to_datetime(out[\"FL_DATE\"].dt.date) + pd.to_timedelta(dep_h, unit=\"h\")\n",
    "    out[\"arr_hour_dt\"] = pd.to_datetime(out[\"FL_DATE\"].dt.date) + pd.to_timedelta(arr_h, unit=\"h\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def _validate_flights(df_flights: pd.DataFrame) -> None:\n",
    "    \"\"\"Ensure flights DF has the required columns.\"\"\"\n",
    "    required = {\"FL_DATE\", \"ORIGIN\", \"DEST\", \"CRS_DEP_TIME\", \"CRS_ARR_TIME\"}\n",
    "    missing = required - set(df_flights.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "\n",
    "def _load_airports(airports_csv: str) -> pd.DataFrame:\n",
    "    \"\"\"Load airports CSV (must include IATA, lat, lon). Returns DF indexed by IATA with ['lat','lon'].\"\"\"\n",
    "    ap = pd.read_csv(airports_csv)\n",
    "    need = {\"IATA\", \"lat\", \"lon\"}\n",
    "    miss = need - set(ap.columns)\n",
    "    if miss:\n",
    "        raise ValueError(f\"Airports CSV must contain columns: {need}. Missing: {miss}\")\n",
    "    ap = ap.dropna(subset=[\"IATA\", \"lat\", \"lon\"]).drop_duplicates(\"IATA\").set_index(\"IATA\")[[\"lat\", \"lon\"]]\n",
    "    return ap\n",
    "\n",
    "\n",
    "def _build_needed_pairs(df: pd.DataFrame, ap: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"From flights with hour keys, build unique (IATA, year) pairs and attach lat/lon.\"\"\"\n",
    "    dep_keys = df[[\"ORIGIN\", \"dep_hour_dt\"]].rename(columns={\"ORIGIN\": \"IATA\", \"dep_hour_dt\": \"ts\"})\n",
    "    arr_keys = df[[\"DEST\", \"arr_hour_dt\"]].rename(columns={\"DEST\": \"IATA\", \"arr_hour_dt\": \"ts\"})\n",
    "    all_keys = pd.concat([dep_keys, arr_keys], ignore_index=True).dropna().drop_duplicates()\n",
    "    all_keys[\"year\"] = all_keys[\"ts\"].dt.year\n",
    "    pairs = all_keys[[\"IATA\", \"year\"]].drop_duplicates().join(ap, on=\"IATA\", how=\"inner\")\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def _fetch_airport_year_raw(iata: str, lat: float, lon: float, year: int,\n",
    "                            cache_dir: Path, force_refresh: bool=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch RAW hourly weather for one (airport, year) from Meteostat.\n",
    "    Caches to Parquet as {IATA}_{year}_RAW.parquet with columns: ts, prcp, temp, wspd, IATA\n",
    "    \"\"\"\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fpath = cache_dir / f\"{iata}_{year}_RAW.parquet\"\n",
    "\n",
    "    if (not force_refresh) and fpath.exists():\n",
    "        return pd.read_parquet(fpath)\n",
    "\n",
    "    p = Point(float(lat), float(lon))\n",
    "    start = datetime(int(year), 1, 1)\n",
    "    end   = datetime(int(year), 12, 31, 23, 59, 59)\n",
    "\n",
    "    wx = Hourly(p, start, end).fetch()  # index 'time' (UTC, hourly)\n",
    "    if wx.empty:\n",
    "        raw = pd.DataFrame(columns=[\"ts\", \"prcp\", \"temp\", \"wspd\", \"IATA\"])\n",
    "    else:\n",
    "        raw = wx.reset_index().rename(columns={\"time\": \"ts\"})\n",
    "        raw = raw[[\"ts\", \"prcp\", \"temp\", \"wspd\"]]\n",
    "        raw[\"IATA\"] = iata\n",
    "\n",
    "    raw.to_parquet(fpath, index=False)\n",
    "    return raw\n",
    "\n",
    "\n",
    "def _fetch_all_raw_with_progress(pairs: pd.DataFrame, cache_dir: Path,\n",
    "                                 max_workers: int, force_refresh: bool) -> pd.DataFrame:\n",
    "    \"\"\"Parallel fetch raw weather for all (IATA, year) pairs with a progress bar.\"\"\"\n",
    "    parts = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = {\n",
    "            ex.submit(_fetch_airport_year_raw, r.IATA, r.lat, r.lon, int(r.year), cache_dir, force_refresh): (r.IATA, int(r.year))\n",
    "            for r in pairs.itertuples(index=False)\n",
    "        }\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Fetching Meteostat raw (airport-year)\"):\n",
    "            parts.append(fut.result())\n",
    "\n",
    "    if parts:\n",
    "        out = pd.concat(parts, ignore_index=True)\n",
    "        out.drop_duplicates(subset=[\"IATA\", \"ts\"], inplace=True)\n",
    "        return out\n",
    "    return pd.DataFrame(columns=[\"ts\", \"prcp\", \"temp\", \"wspd\", \"IATA\"])\n",
    "\n",
    "\n",
    "def _compute_flags_from_raw(raw_df: pd.DataFrame, wind_kmh_threshold: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute binary flags from raw weather:\n",
    "      rain = prcp > 0\n",
    "      ice  = temp <= 0 and prcp > 0\n",
    "      wind = wspd >= threshold\n",
    "    Returns: ['ts','rain','ice','wind','IATA']\n",
    "    \"\"\"\n",
    "    if raw_df.empty:\n",
    "        return pd.DataFrame(columns=[\"ts\", \"rain\", \"ice\", \"wind\", \"IATA\"])\n",
    "\n",
    "    out = raw_df.copy()\n",
    "    out[\"rain\"] = (out[\"prcp\"].fillna(0) > 0).astype(\"int8\")\n",
    "    out[\"ice\"]  = ((out[\"temp\"].fillna(99) <= 0) & (out[\"prcp\"].fillna(0) > 0)).astype(\"int8\")\n",
    "    out[\"wind\"] = (out[\"wspd\"].fillna(0) >= wind_kmh_threshold).astype(\"int8\")\n",
    "    return out[[\"ts\", \"rain\", \"ice\", \"wind\", \"IATA\"]]\n",
    "\n",
    "\n",
    "def _merge_flags(df: pd.DataFrame, wx_flags: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Merge dep/arr flags into flights by (IATA, hour-key) and add *_missing columns.\"\"\"\n",
    "    # Departure merge\n",
    "    dep = (\n",
    "        df[[\"ORIGIN\", \"dep_hour_dt\"]]\n",
    "        .rename(columns={\"ORIGIN\": \"IATA\", \"dep_hour_dt\": \"ts\"})\n",
    "        .merge(wx_flags, on=[\"IATA\", \"ts\"], how=\"left\")\n",
    "        .rename(columns={\"rain\": \"dep_rain\", \"ice\": \"dep_ice\", \"wind\": \"dep_wind\"})\n",
    "    )\n",
    "    # Arrival merge\n",
    "    arr = (\n",
    "        df[[\"DEST\", \"arr_hour_dt\"]]\n",
    "        .rename(columns={\"DEST\": \"IATA\", \"arr_hour_dt\": \"ts\"})\n",
    "        .merge(wx_flags, on=[\"IATA\", \"ts\"], how=\"left\")\n",
    "        .rename(columns={\"rain\": \"arr_rain\", \"ice\": \"arr_ice\", \"wind\": \"arr_wind\"})\n",
    "    )\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"dep_rain\"], out[\"dep_ice\"], out[\"dep_wind\"] = dep[\"dep_rain\"].values, dep[\"dep_ice\"].values, dep[\"dep_wind\"].values\n",
    "    out[\"arr_rain\"], out[\"arr_ice\"], out[\"arr_wind\"] = arr[\"arr_rain\"].values, arr[\"arr_ice\"].values, arr[\"arr_wind\"].values\n",
    "\n",
    "    for col in [\"dep_rain\", \"dep_ice\", \"dep_wind\", \"arr_rain\", \"arr_ice\", \"arr_wind\"]:\n",
    "        out[f\"{col}_missing\"] = out[col].isna().astype(\"int8\")\n",
    "        out[col] = out[col].fillna(0).astype(\"int8\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------------- #\n",
    "# Orchestrator (public entry point)\n",
    "# ---------------------------------- #\n",
    "\n",
    "def enrich_with_weather_flags(\n",
    "    df_flights: pd.DataFrame,\n",
    "    airports_csv: str = \"../data/raw/airports.csv\",  # Must contain IATA, lat, lon\n",
    "    cache_dir: str = \"weather_cache\",                # Where to save raw weather data\n",
    "    wind_kmh_threshold: int = 30,                    # Wind speed threshold for \"wind\" flag\n",
    "    max_workers: int = 12,                           # Parallel fetch threads\n",
    "    force_refresh: bool = False                      # If True, ignore cache and re-download\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    High-level pipeline:\n",
    "      1) Validate inputs\n",
    "      2) Add dep/arr hour keys\n",
    "      3) Load airports & filter flights to known airports\n",
    "      4) Build unique (IATA, year) fetch list\n",
    "      5) Download (or load cached) RAW weather per airport-year\n",
    "      6) Compute rain/ice/wind flags from RAW\n",
    "      7) Merge flags back into flights\n",
    "    \"\"\"\n",
    "    # 1) Validate\n",
    "    _validate_flights(df_flights)\n",
    "\n",
    "    # 2) Hour keys\n",
    "    df_keyed = _add_hour_keys(df_flights)\n",
    "\n",
    "    # 3) Airports & filter\n",
    "    ap = _load_airports(airports_csv)\n",
    "    df_keyed = df_keyed[df_keyed[\"ORIGIN\"].isin(ap.index) & df_keyed[\"DEST\"].isin(ap.index)].copy()\n",
    "\n",
    "    # 4) Needed pairs\n",
    "    pairs = _build_needed_pairs(df_keyed, ap)\n",
    "\n",
    "    # 5) Fetch RAW with progress\n",
    "    cache_path = Path(cache_dir)\n",
    "    raw_weather = _fetch_all_raw_with_progress(pairs, cache_path, max_workers, force_refresh)\n",
    "\n",
    "    # 6) Compute flags\n",
    "    wx_flags = _compute_flags_from_raw(raw_weather, wind_kmh_threshold)\n",
    "\n",
    "    # 7) Merge to flights\n",
    "    df_enriched = _merge_flags(df_keyed, wx_flags)\n",
    "    return df_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = enrich_with_weather_flags(\n",
    "    df_flights=df,\n",
    "    airports_csv=\"../data/raw/airports.csv\",    # Path to airports CSV\n",
    "    cache_dir=\"weather_cache\",           # Cache directory for raw weather\n",
    "    wind_kmh_threshold=30,                       # Wind speed threshold for \"wind\" flag\n",
    "    max_workers=12,                              # Number of parallel fetch threads\n",
    "    force_refresh=True                          # If True, ignore cache and re-download\n",
    ")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dep_rain.value_counts()  # Check for missing values in arr_wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at percentages of flags\n",
    "# List of weather flags\n",
    "flags = [\"dep_rain\", \"dep_ice\", \"dep_wind\", \"arr_rain\", \"arr_ice\", \"arr_wind\"]\n",
    "\n",
    "# Calculate counts and percentages\n",
    "for col in flags:\n",
    "    counts = df[col].value_counts().sort_index()\n",
    "    total = counts.sum()\n",
    "    pct = (counts / total * 100).round(2)\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    for val in counts.index:\n",
    "        print(f\"  {val}: {counts[val]} flights ({pct[val]}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## We will now add some temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add temporal features to the DataFrame:\n",
    "      - day_of_week (0=Mon, 6=Sun)\n",
    "      - month (1-12)\n",
    "      - hour_of_day (0-23) from CRS_DEP_TIME\n",
    "      - is_bank_holiday (US federal holiday)\n",
    "    \n",
    "    Parameters:\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing 'FL_DATE' and 'CRS_DEP_TIME' columns.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame : DataFrame with added columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"FL_DATE\"] = pd.to_datetime(df[\"FL_DATE\"])\n",
    "\n",
    "    # Day of week & month\n",
    "    df[\"day_of_week\"] = df[\"FL_DATE\"].dt.dayofweek\n",
    "    df[\"month\"] = df[\"FL_DATE\"].dt.month\n",
    "\n",
    "    # Scheduled departure hour\n",
    "    df[\"hour_of_day\"] = df[\"CRS_DEP_TIME\"].apply(\n",
    "        lambda x: int(str(int(x)).zfill(4)[:2]) if pd.notnull(x) else 0\n",
    "    )\n",
    "\n",
    "    # US Bank Holidays\n",
    "    start_year, end_year = df[\"FL_DATE\"].dt.year.min(), df[\"FL_DATE\"].dt.year.max()\n",
    "    us_holidays = holidays.US(years=range(start_year, end_year + 1))\n",
    "    df[\"is_bank_holiday\"] = df[\"FL_DATE\"].isin(us_holidays).astype(\"int8\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_temporal_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# Drop columns that arent needed\n",
    "- such as those with information only available after flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that arent needed\n",
    "# such as those with information only available after flight\n",
    "# and columns that are clearly redundant like FL_DATE, CRS_DEP_TIME, CRS_ARR_TIME\n",
    "cols_to_drop = [\n",
    "            # Identifiers\n",
    "            \"FL_NUMBER\",\n",
    "            \"AIRLINE_DOT\",\n",
    "            \"AIRLINE_CODE\",\n",
    "            \"DOT_CODE\",\n",
    "            # Location names (redundant with IATA codes)\n",
    "            \"ORIGIN_CITY\",\n",
    "            \"DEST_CITY\",\n",
    "            # oout of scope / not useful for prediction\n",
    "            \"CRS_DEP_TIME\",\n",
    "            \"CRS_ARR_TIME\",\n",
    "            \"CRS_ELAPSED_TIME\",\n",
    "            \"CANCELLED\",\n",
    "            \"DIVERTED\",\n",
    "            \"CRS_ELAPSED_TIME\"\n",
    "            # Future / leakage features\n",
    "            \"DEP_TIME\",\n",
    "            \"DEP_DELAY\",\n",
    "            \"TAXI_OUT\",\n",
    "            \"WHEELS_OFF\",\n",
    "            \"WHEELS_ON\",\n",
    "            \"TAXI_IN\",\n",
    "            \"ARR_TIME\",\n",
    "            \"ARR_DELAY\",\n",
    "            \"CANCELLATION_CODE\",\n",
    "            \"ELAPSED_TIME\",\n",
    "            \"AIR_TIME\",\n",
    "            \"DELAY_DUE_CARRIER\",\n",
    "            \"DELAY_DUE_WEATHER\",\n",
    "            \"DELAY_DUE_NAS\",\n",
    "            \"DELAY_DUE_SECURITY\",\n",
    "            \"DELAY_DUE_LATE_AIRCRAFT\",\n",
    "            \"DEP_TIME\",\n",
    "            # Raw datetime keys (already used to make features)\n",
    "            \"FL_DATE\",\n",
    "            \"dep_hour_dt\",\n",
    "            \"arr_hour_dt\",\n",
    "        ]\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df.drop(columns=cols_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only numeric columns\n",
    "numeric_df = df_dropped.select_dtypes(include=['number'])\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    numeric_df.corr(),\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap='coolwarm',\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.8}\n",
    ")\n",
    "plt.title('Correlation Matrix of Numeric Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.drop(columns=[\n",
    "    \"dep_rain_missing\",\n",
    "    \"dep_ice_missing\",\n",
    "    \"dep_wind_missing\",\n",
    "    \"arr_rain_missing\",\n",
    "    \"arr_ice_missing\",\n",
    "    \"arr_wind_missing\"\n",
    "], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only numeric columns\n",
    "numeric_df = df_test.select_dtypes(include=['number'])\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    numeric_df.corr(),\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap='coolwarm',\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.8}\n",
    ")\n",
    "plt.title('Correlation Matrix of Numeric Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "# Save final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(\"../test_data/data/processed/preprocessed_data.csv\"), exist_ok=True)\n",
    "df_dropped.to_csv(\"../test_data/data/processed/preprocessed_data.csv\", index=False)\n",
    "print(f\"Preprocessed data saved to ../test_date/data/processed/preprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make directory if it does not exist\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "df_dropped.to_csv(\"../data/processed/preprocessed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"/Users/harish/Desktop/Aero-delay-prediction/test_date/data/processed/preprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at percentages of flags\n",
    "# List of weather flags\n",
    "flags = [\"dep_rain\", \"dep_ice\", \"dep_wind\", \"arr_rain\", \"arr_ice\", \"arr_wind\"]\n",
    "\n",
    "# Calculate counts and percentages\n",
    "for col in flags:\n",
    "    counts = df_test[col].value_counts().sort_index()\n",
    "    total = counts.sum()\n",
    "    pct = (counts / total * 100).round(2)\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    for val in counts.index:\n",
    "        print(f\"  {val}: {counts[val]} flights ({pct[val]}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.arr_wind.isna().sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.arr_ice.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
