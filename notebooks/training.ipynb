{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "df986d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.2 in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: meteostat==1.6.7 in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 2)) (1.6.7)\n",
      "Requirement already satisfied: pyarrow==12.0.0 in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 3)) (12.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: numpy in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 5)) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: lightgbm in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 7)) (4.6.0)\n",
      "Requirement already satisfied: joblib in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: kagglehub in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 9)) (0.3.12)\n",
      "Requirement already satisfied: holidays in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 10)) (0.78)\n",
      "Requirement already satisfied: seaborn in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 11)) (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 12)) (3.7.2)\n",
      "Requirement already satisfied: python-dotenv in /Users/harish/miniconda3/lib/python3.11/site-packages (from -r ../requirements.txt (line 13)) (1.1.1)\n",
      "Collecting xgboost (from -r ../requirements.txt (line 14))\n",
      "  Downloading xgboost-3.0.3-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/harish/miniconda3/lib/python3.11/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/harish/miniconda3/lib/python3.11/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/harish/miniconda3/lib/python3.11/site-packages (from pandas==2.2.2->-r ../requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/harish/miniconda3/lib/python3.11/site-packages (from scikit-learn->-r ../requirements.txt (line 6)) (1.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/harish/miniconda3/lib/python3.11/site-packages (from scikit-learn->-r ../requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: packaging in /Users/harish/miniconda3/lib/python3.11/site-packages (from kagglehub->-r ../requirements.txt (line 9)) (25.0)\n",
      "Requirement already satisfied: pyyaml in /Users/harish/miniconda3/lib/python3.11/site-packages (from kagglehub->-r ../requirements.txt (line 9)) (6.0)\n",
      "Requirement already satisfied: requests in /Users/harish/miniconda3/lib/python3.11/site-packages (from kagglehub->-r ../requirements.txt (line 9)) (2.32.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/harish/miniconda3/lib/python3.11/site-packages (from matplotlib->-r ../requirements.txt (line 12)) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/harish/miniconda3/lib/python3.11/site-packages (from matplotlib->-r ../requirements.txt (line 12)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/harish/miniconda3/lib/python3.11/site-packages (from matplotlib->-r ../requirements.txt (line 12)) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/harish/miniconda3/lib/python3.11/site-packages (from matplotlib->-r ../requirements.txt (line 12)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/harish/miniconda3/lib/python3.11/site-packages (from matplotlib->-r ../requirements.txt (line 12)) (10.0.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/harish/miniconda3/lib/python3.11/site-packages (from matplotlib->-r ../requirements.txt (line 12)) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /Users/harish/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r ../requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/harish/miniconda3/lib/python3.11/site-packages (from requests->kagglehub->-r ../requirements.txt (line 9)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/harish/miniconda3/lib/python3.11/site-packages (from requests->kagglehub->-r ../requirements.txt (line 9)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/harish/miniconda3/lib/python3.11/site-packages (from requests->kagglehub->-r ../requirements.txt (line 9)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/harish/miniconda3/lib/python3.11/site-packages (from requests->kagglehub->-r ../requirements.txt (line 9)) (2024.2.2)\n",
      "Downloading xgboost-3.0.3-py3-none-macosx_12_0_arm64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install required packages\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30a8ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports & data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9403a3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance (all): Counter({0: 1104914, 1: 296762})\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — imports & load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    classification_report\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# Load your preprocessed CSV\n",
    "df = pd.read_csv(\"../test_date/data/processed/preprocessed_data.csv\")  # <- adjust if needed\n",
    "\n",
    "TARGET = \"DELAY_FLAG_15\"\n",
    "cat_cols = [\"AIRLINE\", \"ORIGIN\", \"DEST\"]\n",
    "num_cols = [\n",
    "    \"DISTANCE\", \"day_of_week\", \"month\", \"hour_of_day\", \"is_bank_holiday\",\n",
    "    \"dep_rain\", \"dep_ice\", \"dep_wind\", \"arr_rain\", \"arr_ice\", \"arr_wind\"\n",
    "]\n",
    "\n",
    "use_cols = [c for c in cat_cols + num_cols + [TARGET] if c in df.columns]\n",
    "df = df[use_cols].dropna()\n",
    "\n",
    "X = df[cat_cols + num_cols]\n",
    "y = df[TARGET].astype(int)\n",
    "\n",
    "print(\"Class balance (all):\", Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e6aad826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN original: Counter({0: 883931, 1: 237409})\n",
      "TRAIN balanced: Counter({1: 237409, 0: 237409})\n",
      "TEST: Counter({0: 220983, 1: 59353})\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — split + undersample TRAIN only\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "def undersample_even_split(X_tr, y_tr, random_state=42):\n",
    "    cnt = Counter(y_tr)\n",
    "    n_min = min(cnt.values())\n",
    "    idx_0 = np.where(y_tr.values == 0)[0]\n",
    "    idx_1 = np.where(y_tr.values == 1)[0]\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    if len(idx_0) > len(idx_1):\n",
    "        keep = np.concatenate([rng.choice(idx_0, n_min, replace=False), idx_1])\n",
    "    else:\n",
    "        keep = np.concatenate([idx_0, rng.choice(idx_1, n_min, replace=False)])\n",
    "    rng.shuffle(keep)\n",
    "    return X_tr.iloc[keep].reset_index(drop=True), y_tr.iloc[keep].reset_index(drop=True)\n",
    "\n",
    "X_tr_bal, y_tr_bal = undersample_even_split(X_train, y_train, random_state=42)\n",
    "print(\"TRAIN original:\", Counter(y_train))\n",
    "print(\"TRAIN balanced:\", Counter(y_tr_bal))\n",
    "print(\"TEST:\", Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a55950a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting encoder...\n",
      "Transforming...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — fit the encoder, transform to matrices\n",
    "from sklearn.utils.sparsefuncs import inplace_column_scale\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0\n",
    ")\n",
    "\n",
    "print(\"Fitting encoder...\")\n",
    "preprocess.fit(X_tr_bal)\n",
    "print(\"Transforming...\")\n",
    "Xtr_enc = preprocess.transform(X_tr_bal)   # sparse\n",
    "Xte_enc = preprocess.transform(X_test)     # sparse\n",
    "\n",
    "# Optional: you can standardize numeric columns if you want (usually not needed for trees)\n",
    "# Leaving as-is for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06076b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train balance: Counter({0: 883931, 1: 237409})\n",
      "Test balance: Counter({0: 220983, 1: 59353})\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: split\n",
    "X = df[cat_cols + num_cols]\n",
    "y = df[TARGET].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train balance:\", Counter(y_train))\n",
    "print(\"Test balance:\", Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c884f326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost with progress...\n",
      "Early stopping not supported in this XGBoost version. Training without it...\n",
      "[0]\tvalidation_0-aucpr:0.63248\tvalidation_1-aucpr:0.32009\n",
      "[10]\tvalidation_0-aucpr:0.65868\tvalidation_1-aucpr:0.34946\n",
      "[20]\tvalidation_0-aucpr:0.66485\tvalidation_1-aucpr:0.35626\n",
      "[30]\tvalidation_0-aucpr:0.66680\tvalidation_1-aucpr:0.35833\n",
      "[40]\tvalidation_0-aucpr:0.67048\tvalidation_1-aucpr:0.36254\n",
      "[50]\tvalidation_0-aucpr:0.67321\tvalidation_1-aucpr:0.36533\n",
      "[60]\tvalidation_0-aucpr:0.67577\tvalidation_1-aucpr:0.36782\n",
      "[70]\tvalidation_0-aucpr:0.67779\tvalidation_1-aucpr:0.36964\n",
      "[80]\tvalidation_0-aucpr:0.67982\tvalidation_1-aucpr:0.37142\n",
      "[90]\tvalidation_0-aucpr:0.68141\tvalidation_1-aucpr:0.37280\n",
      "[100]\tvalidation_0-aucpr:0.68275\tvalidation_1-aucpr:0.37394\n",
      "[110]\tvalidation_0-aucpr:0.68419\tvalidation_1-aucpr:0.37513\n",
      "[120]\tvalidation_0-aucpr:0.68537\tvalidation_1-aucpr:0.37620\n",
      "[130]\tvalidation_0-aucpr:0.68661\tvalidation_1-aucpr:0.37725\n",
      "[140]\tvalidation_0-aucpr:0.68741\tvalidation_1-aucpr:0.37779\n",
      "[150]\tvalidation_0-aucpr:0.68836\tvalidation_1-aucpr:0.37844\n",
      "[160]\tvalidation_0-aucpr:0.68925\tvalidation_1-aucpr:0.37910\n",
      "[170]\tvalidation_0-aucpr:0.69015\tvalidation_1-aucpr:0.37977\n",
      "[180]\tvalidation_0-aucpr:0.69083\tvalidation_1-aucpr:0.38017\n",
      "[190]\tvalidation_0-aucpr:0.69145\tvalidation_1-aucpr:0.38047\n",
      "[200]\tvalidation_0-aucpr:0.69219\tvalidation_1-aucpr:0.38103\n",
      "[210]\tvalidation_0-aucpr:0.69268\tvalidation_1-aucpr:0.38131\n",
      "[220]\tvalidation_0-aucpr:0.69313\tvalidation_1-aucpr:0.38164\n",
      "[230]\tvalidation_0-aucpr:0.69369\tvalidation_1-aucpr:0.38193\n",
      "[240]\tvalidation_0-aucpr:0.69433\tvalidation_1-aucpr:0.38234\n",
      "[250]\tvalidation_0-aucpr:0.69487\tvalidation_1-aucpr:0.38270\n",
      "[260]\tvalidation_0-aucpr:0.69535\tvalidation_1-aucpr:0.38294\n",
      "[270]\tvalidation_0-aucpr:0.69573\tvalidation_1-aucpr:0.38310\n",
      "[280]\tvalidation_0-aucpr:0.69612\tvalidation_1-aucpr:0.38335\n",
      "[290]\tvalidation_0-aucpr:0.69644\tvalidation_1-aucpr:0.38347\n",
      "[300]\tvalidation_0-aucpr:0.69684\tvalidation_1-aucpr:0.38370\n",
      "[310]\tvalidation_0-aucpr:0.69725\tvalidation_1-aucpr:0.38396\n",
      "[320]\tvalidation_0-aucpr:0.69770\tvalidation_1-aucpr:0.38424\n",
      "[330]\tvalidation_0-aucpr:0.69796\tvalidation_1-aucpr:0.38432\n",
      "[340]\tvalidation_0-aucpr:0.69829\tvalidation_1-aucpr:0.38448\n",
      "[350]\tvalidation_0-aucpr:0.69866\tvalidation_1-aucpr:0.38465\n",
      "[360]\tvalidation_0-aucpr:0.69899\tvalidation_1-aucpr:0.38474\n",
      "[370]\tvalidation_0-aucpr:0.69924\tvalidation_1-aucpr:0.38481\n",
      "[380]\tvalidation_0-aucpr:0.69949\tvalidation_1-aucpr:0.38494\n",
      "[390]\tvalidation_0-aucpr:0.69976\tvalidation_1-aucpr:0.38502\n",
      "[400]\tvalidation_0-aucpr:0.70007\tvalidation_1-aucpr:0.38512\n",
      "[410]\tvalidation_0-aucpr:0.70027\tvalidation_1-aucpr:0.38519\n",
      "[420]\tvalidation_0-aucpr:0.70063\tvalidation_1-aucpr:0.38533\n",
      "[430]\tvalidation_0-aucpr:0.70089\tvalidation_1-aucpr:0.38542\n",
      "[440]\tvalidation_0-aucpr:0.70125\tvalidation_1-aucpr:0.38560\n",
      "[450]\tvalidation_0-aucpr:0.70152\tvalidation_1-aucpr:0.38569\n",
      "[460]\tvalidation_0-aucpr:0.70179\tvalidation_1-aucpr:0.38582\n",
      "[470]\tvalidation_0-aucpr:0.70206\tvalidation_1-aucpr:0.38597\n",
      "[480]\tvalidation_0-aucpr:0.70227\tvalidation_1-aucpr:0.38604\n",
      "[490]\tvalidation_0-aucpr:0.70260\tvalidation_1-aucpr:0.38624\n",
      "[499]\tvalidation_0-aucpr:0.70278\tvalidation_1-aucpr:0.38632\n",
      "Done. Best ntree limit: None\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — train XGBoost with live logs + early stopping (compatible with older versions)\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=500,      # high cap; early stopping will halt earlier\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"aucpr\",    # better for imbalanced data\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "eval_set = [(Xtr_enc, y_tr_bal), (Xte_enc, y_test)]\n",
    "\n",
    "print(\"Training XGBoost with progress...\")\n",
    "trained_with_es = False\n",
    "try:\n",
    "    # Newer versions support early_stopping_rounds + verbose directly\n",
    "    xgb.fit(\n",
    "        Xtr_enc, y_tr_bal,\n",
    "        eval_set=eval_set,\n",
    "        early_stopping_rounds=30,\n",
    "        verbose=10   # print every 10 boosting rounds\n",
    "    )\n",
    "    trained_with_es = True\n",
    "except TypeError:\n",
    "    # Older versions: remove early stopping but keep logs\n",
    "    print(\"Early stopping not supported in this XGBoost version. Training without it...\")\n",
    "    xgb.fit(\n",
    "        Xtr_enc, y_tr_bal,\n",
    "        eval_set=eval_set,\n",
    "        verbose=10\n",
    "    )\n",
    "\n",
    "print(\"Done. Best ntree limit:\", getattr(xgb, \"best_ntree_limit\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4bc396e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: {'accuracy': 0.6504908395639518, 'f1': 0.4283613960163825, 'roc_auc': 0.6911627260247225, 'pr_auc': 0.38633868297701235}\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.865     0.659     0.748    220983\n",
      "           1      0.328     0.619     0.428     59353\n",
      "\n",
      "    accuracy                          0.650    280336\n",
      "   macro avg      0.597     0.639     0.588    280336\n",
      "weighted avg      0.752     0.650     0.681    280336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — evaluate\n",
    "# Use the best_ntree_limit if early stopping was used\n",
    "use_ntree = getattr(xgb, \"best_ntree_limit\", 0) if trained_with_es else 0\n",
    "\n",
    "if use_ntree and use_ntree > 0:\n",
    "    proba = xgb.predict_proba(Xte_enc, ntree_limit=use_ntree)[:, 1]\n",
    "else:\n",
    "    proba = xgb.predict_proba(Xte_enc)[:, 1]\n",
    "\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, pred),\n",
    "    \"f1\": f1_score(y_test, pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, proba),\n",
    "    \"pr_auc\": average_precision_score(y_test, proba),\n",
    "}\n",
    "print(\"Metrics:\", metrics)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "90452845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved models/preprocess.pkl and models/xgb_model.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — save artifacts (encoder + model)\n",
    "joblib.dump(preprocess, \"models/preprocess.pkl\")\n",
    "# Save the XGBoost model as JSON (version-stable)\n",
    "xgb.save_model(\"models/xgb_model.json\")\n",
    "print(\"Saved models/preprocess.pkl and models/xgb_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cb453c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33312893, 0.494758  , 0.6988505 , 0.3631203 , 0.6668398 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 7 — quick inference helper (optional)\n",
    "def predict_delay_proba(batch_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    batch_df must have the same feature columns: cat_cols + num_cols\n",
    "    \"\"\"\n",
    "    enc = joblib.load(\"models/preprocess.pkl\")\n",
    "    from xgboost import XGBClassifier\n",
    "    mdl = XGBClassifier()\n",
    "    mdl.load_model(\"models/xgb_model.json\")\n",
    "    Xb = enc.transform(batch_df[cat_cols + num_cols])\n",
    "    return mdl.predict_proba(Xb)[:, 1]\n",
    "\n",
    "# Example:\n",
    "sample = X_test.iloc[:5].copy()\n",
    "predict_delay_proba(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
