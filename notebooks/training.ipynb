{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports & data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — imports & load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, average_precision_score,\n",
    "    classification_report\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# Load your preprocessed CSV\n",
    "df = pd.read_csv(\"../test_date/data/processed/preprocessed_data.csv\")  # <- adjust if needed\n",
    "\n",
    "TARGET = \"DELAY_FLAG_30\"\n",
    "cat_cols = [\"AIRLINE\", \"ORIGIN\", \"DEST\"]\n",
    "num_cols = [\n",
    "    \"DISTANCE\", \"day_of_week\", \"month\", \"hour_of_day\", \"is_bank_holiday\",\n",
    "    \"dep_rain\", \"dep_ice\", \"dep_wind\", \"arr_rain\", \"arr_ice\", \"arr_wind\"\n",
    "]\n",
    "\n",
    "use_cols = [c for c in cat_cols + num_cols + [TARGET] if c in df.columns]\n",
    "df = df[use_cols].dropna()\n",
    "\n",
    "X = df[cat_cols + num_cols]\n",
    "y = df[TARGET].astype(int)\n",
    "\n",
    "print(\"Class balance (all):\", Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — split + undersample TRAIN only\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "def undersample_even_split(X_tr, y_tr, random_state=42):\n",
    "    cnt = Counter(y_tr)\n",
    "    n_min = min(cnt.values())\n",
    "    idx_0 = np.where(y_tr.values == 0)[0]\n",
    "    idx_1 = np.where(y_tr.values == 1)[0]\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    if len(idx_0) > len(idx_1):\n",
    "        keep = np.concatenate([rng.choice(idx_0, n_min, replace=False), idx_1])\n",
    "    else:\n",
    "        keep = np.concatenate([idx_0, rng.choice(idx_1, n_min, replace=False)])\n",
    "    rng.shuffle(keep)\n",
    "    return X_tr.iloc[keep].reset_index(drop=True), y_tr.iloc[keep].reset_index(drop=True)\n",
    "\n",
    "X_tr_bal, y_tr_bal = undersample_even_split(X_train, y_train, random_state=42)\n",
    "print(\"TRAIN original:\", Counter(y_train))\n",
    "print(\"TRAIN balanced:\", Counter(y_tr_bal))\n",
    "print(\"TEST:\", Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — fit the encoder, transform to matrices\n",
    "from sklearn.utils.sparsefuncs import inplace_column_scale\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0\n",
    ")\n",
    "\n",
    "print(\"Fitting encoder...\")\n",
    "preprocess.fit(X_tr_bal)\n",
    "print(\"Transforming...\")\n",
    "Xtr_enc = preprocess.transform(X_tr_bal)   # sparse\n",
    "Xte_enc = preprocess.transform(X_test)     # sparse\n",
    "\n",
    "# Optional: you can standardize numeric columns if you want (usually not needed for trees)\n",
    "# Leaving as-is for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: split\n",
    "X = df[cat_cols + num_cols]\n",
    "y = df[TARGET].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train balance:\", Counter(y_train))\n",
    "print(\"Test balance:\", Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — train XGBoost with live logs + early stopping (compatible with older versions)\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=500,      # high cap; early stopping will halt earlier\n",
    "    learning_rate=1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"aucpr\",    # better for imbalanced data\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "eval_set = [(Xtr_enc, y_tr_bal), (Xte_enc, y_test)]\n",
    "\n",
    "print(\"Training XGBoost with progress...\")\n",
    "trained_with_es = False\n",
    "try:\n",
    "    # Newer versions support early_stopping_rounds + verbose directly\n",
    "    xgb.fit(\n",
    "        Xtr_enc, y_tr_bal,\n",
    "        eval_set=eval_set,\n",
    "        early_stopping_rounds=30,\n",
    "        verbose=10   # print every 10 boosting rounds\n",
    "    )\n",
    "    trained_with_es = True\n",
    "except TypeError:\n",
    "    # Older versions: remove early stopping but keep logs\n",
    "    print(\"Early stopping not supported in this XGBoost version. Training without it...\")\n",
    "    xgb.fit(\n",
    "        Xtr_enc, y_tr_bal,\n",
    "        eval_set=eval_set,\n",
    "        verbose=10\n",
    "    )\n",
    "\n",
    "print(\"Done. Best ntree limit:\", getattr(xgb, \"best_ntree_limit\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — evaluate\n",
    "# Use the best_ntree_limit if early stopping was used\n",
    "use_ntree = getattr(xgb, \"best_ntree_limit\", 0) if trained_with_es else 0\n",
    "\n",
    "if use_ntree and use_ntree > 0:\n",
    "    proba = xgb.predict_proba(Xte_enc, ntree_limit=use_ntree)[:, 1]\n",
    "else:\n",
    "    proba = xgb.predict_proba(Xte_enc)[:, 1]\n",
    "\n",
    "pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, pred),\n",
    "    \"f1\": f1_score(y_test, pred),\n",
    "    \"roc_auc\": roc_auc_score(y_test, proba),\n",
    "    \"pr_auc\": average_precision_score(y_test, proba),\n",
    "}\n",
    "print(\"Metrics:\", metrics)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — save artifacts (encoder + model)\n",
    "joblib.dump(preprocess, \"models/preprocess.pkl\")\n",
    "# Save the XGBoost model as JSON (version-stable)\n",
    "xgb.save_model(\"models/xgb_model.json\")\n",
    "print(\"Saved models/preprocess.pkl and models/xgb_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — quick inference helper (optional)\n",
    "def predict_delay_proba(batch_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    batch_df must have the same feature columns: cat_cols + num_cols\n",
    "    \"\"\"\n",
    "    enc = joblib.load(\"models/preprocess.pkl\")\n",
    "    from xgboost import XGBClassifier\n",
    "    mdl = XGBClassifier()\n",
    "    mdl.load_model(\"models/xgb_model.json\")\n",
    "    Xb = enc.transform(batch_df[cat_cols + num_cols])\n",
    "    return mdl.predict_proba(Xb)[:, 1]\n",
    "\n",
    "# Example:\n",
    "sample = X_test.iloc[:5].copy()\n",
    "predict_delay_proba(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
